{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7974971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import sys\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('articles1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd527112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Make a DataFrame with articles by our chosen authors\n",
    "# # Include author names and article titles.\n",
    "\n",
    "# # Make a list of the 10 chosen author names\n",
    "# names = data.author.value_counts()[data.author.value_counts()>100][-30:].index.tolist()\n",
    "\n",
    "# # DataFrame for articles of all chosen authors\n",
    "# authors_data = pd.DataFrame()\n",
    "# for name in names:\n",
    "#     # Select each author's data\n",
    "#     articles = data[data.author==name][:100][['title','content','author']]\n",
    "#     # Append it to the DataFrame\n",
    "#     authors_data = authors_data.append(articles)\n",
    "\n",
    "# authors_data = authors_data.reset_index().drop('index',1)\n",
    "    \n",
    "# authors_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look for duplicates\n",
    "# print('Number of articles:',authors_data.shape[0])\n",
    "# print('Unique articles:',len(np.unique(authors_data.index)))\n",
    "\n",
    "# # Number of authors\n",
    "# print('Unique authors:',len(np.unique(authors_data.author)))\n",
    "# print('')\n",
    "# print('Articles by author:\\n')\n",
    "\n",
    "# # Articles counts by author\n",
    "# print(authors_data.author.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!{sys.executable} -m spacy download en\n",
    "\n",
    "# t0 = time()\n",
    "\n",
    "# # Load spacy NLP object\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# # A list to store common words by all authors\n",
    "# common_words = []\n",
    "\n",
    "# # A dictionary to store the spacy_doc object of each author\n",
    "# authors_docs = {}\n",
    "\n",
    "# for name in names:\n",
    "#     # Corpus is all the text written by that author\n",
    "#     corpus = \"\"\n",
    "#     # Grab all rows of current author, along the 'content' column\n",
    "#     author_content = authors_data.loc[authors_data.author==name,'content']\n",
    "    \n",
    "#     # Merge all articles in to the author's corpus\n",
    "#     for article in author_content:\n",
    "#         corpus = corpus + article\n",
    "#     # Let Spacy parse the author's body of text\n",
    "#     doc = nlp(corpus)\n",
    "    \n",
    "#     # Store the doc in the dictionary\n",
    "#     authors_docs[name] = doc\n",
    "        \n",
    "#     # Filter out punctuation and stop words.\n",
    "#     lemmas = [token.lemma_ for token in doc\n",
    "#                 if not token.is_punct and not token.is_stop]\n",
    "        \n",
    "#     # Return the most common words of that author's corpus.\n",
    "#     bow = [item[0] for item in Counter(lemmas).most_common(1000)]\n",
    "    \n",
    "#     # Add them to the list of words by all authors.\n",
    "#     for word in bow:\n",
    "#         common_words.append(word)\n",
    "\n",
    "# # Eliminate duplicates\n",
    "# common_words = set(common_words)\n",
    "    \n",
    "# print('Total number of common words:',len(common_words))\n",
    "# print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's see our 10 authors in the dictionary\n",
    "# lengths = []\n",
    "# for k,v in authors_docs.items():\n",
    "#     print(k,'corpus contains',len(v),' words.')\n",
    "#     lengths.append(len(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b24c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for lower case words\n",
    "# common_words = pd.Series(pd.DataFrame(columns=common_words).columns)\n",
    "# print('Count of all common_words:',len(common_words))\n",
    "# print('Count of lowercase common_words:',np.sum([word.islower() for word in common_words]))\n",
    "\n",
    "# # Turn all common_words into lower case\n",
    "# common_words = [word.lower() for word in common_words]\n",
    "# print('Count of lowercase common_words (After Conversion):',np.sum([word.islower() for word in common_words]))\n",
    "\n",
    "\n",
    "# # We must remove these in to avoid conflicts with existing features.\n",
    "# if 'author' in common_words:\n",
    "#     common_words.remove('author')\n",
    "# if 'title' in common_words:\n",
    "#     common_words.remove('title')\n",
    "# if 'content' in common_words:\n",
    "#     common_words.remove('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e31326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of times a common_word appears in each article\n",
    "# # (about 3Hrs processing)\n",
    "\n",
    "# bow_counts = pd.DataFrame()\n",
    "# for name in names:\n",
    "#     # Select X articles of that author\n",
    "#     articles = authors_data.loc[authors_data.author==name,:][:50]\n",
    "#     bow_counts = bow_counts.append(articles)\n",
    "# bow_counts = bow_counts.reset_index().drop('index',1)\n",
    "\n",
    "# # Use common_words as the columns of a temporary DataFrame\n",
    "# df = pd.DataFrame(columns=common_words)\n",
    "\n",
    "# # Join BOW features with the author's content\n",
    "# bow_counts = bow_counts.join(df)\n",
    "\n",
    "# # Initialize rows with zeroes\n",
    "# bow_counts = bow_counts.loc[:,~bow_counts.columns.duplicated()].copy()\n",
    "# bow_counts.loc[:,common_words] = 0\n",
    "\n",
    "# # Fill the DataFrame with counts of each feature in each article\n",
    "# t0 = time()\n",
    "# for i, article in enumerate(bow_counts.content):\n",
    "#     doc = nlp(article)\n",
    "#     for token in doc:\n",
    "#         if token.lemma_.lower() in common_words:\n",
    "#             bow_counts.loc[i,token.lemma_.lower()] += 1\n",
    "#     # Print a message every X articles\n",
    "#     if i % 50 == 0:\n",
    "#         if time()-t0 < 3600: # if less than an hour in seconds\n",
    "#             print(\"Article \",i,\" done after \",(time()-t0)/60,' minutes.')\n",
    "#         else:\n",
    "#             print(\"Article \",i,\" done after \",(time()-t0)/60/60,' hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves the long-awaited data into a pickle file for easy recovery\n",
    "#bow_counts.to_pickle('bow_counts')\n",
    "# Read it back in with the following\n",
    "bow_counts = pd.read_pickle('bow_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "newy = np.zeros(len(y))\n",
    "last = y[0]\n",
    "for i in range(len(y)):\n",
    "    if y[i] != last:\n",
    "        counter+=1\n",
    "    newy[i] = counter\n",
    "    last = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bow_counts['author']\n",
    "X = bow_counts.drop(['content','author','title'], 1)\n",
    "X = X.loc[:,~X.columns.duplicated()].copy()\n",
    "X = X.to_numpy()\n",
    "N, M = X.shape\n",
    "import torch.nn as nn\n",
    "#X = scipy.stats.zscore(X)\n",
    "\n",
    "n_hidden_units = 2     # number of hidden units\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 10000         # stop criterion 2 (max epochs in training)\n",
    "\n",
    "# K-fold crossvalidation\n",
    "#K = 3                   # only five folds to speed up this example\n",
    "#CV = model_selection.KFold(K, shuffle=True)\n",
    "model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Sigmoid(), # final tranfer function\n",
    "                    )\n",
    "#loss_fn = torch.nn.MSELoss\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self,D_in,H,D_out):\n",
    "#         super(Net,self).__init__()\n",
    "#         self.linear1=nn.Linear(D_in,H)\n",
    "#         self.linear2=nn.Linear(H,D_out)\n",
    "\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=torch.sigmoid(self.linear1(x))  \n",
    "#         x=self.linear2(x)\n",
    "#         return x\n",
    "# model=Net(M,25,30)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,D_in,H,H2,D_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.linear3=nn.Linear(H2,D_out)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=F.relu(self.linear2(x))  \n",
    "        x=self.linear3(x)\n",
    "        return x\n",
    "model=Net(M,120,50,30)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_val, y, y_val = train_test_split(X, newy, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x.reshape(-1, x.shape[1]).astype('float32')\n",
    "y_train = y\n",
    "\n",
    "x_val = x_val.reshape(-1, x_val.shape[1]).astype('float32')\n",
    "y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = torch.from_numpy(x_val)\n",
    "y_val = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x=torch.from_numpy(x_train)\n",
    "        self.y=torch.from_numpy(y_train)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "data_set=Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805201ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=DataLoader(dataset=data_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5000\n",
    "loss_list=[]\n",
    "\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in trainloader:\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss, da Cross Entropy benutzt wird muss ich in den loss Klassen vorhersagen, \n",
    "        # also Wahrscheinlichkeit pro Klasse. Das mach torch.max(y,1)[1])\n",
    "        loss=loss_fn(z,y.long())\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model(x_val)\n",
    "yhat=torch.max(z.data,1)\n",
    "errors = 0\n",
    "for i in range(len(y_val)):\n",
    "    if yhat[1][i] != y_val[i]:\n",
    "        errors+=1\n",
    "print(errors, 'errors out of', len(y_val), 'inputs giving us an error rate of', errors/len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700eae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_val)):\n",
    "    print(int(yhat[1][i]), int(y_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efac55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
