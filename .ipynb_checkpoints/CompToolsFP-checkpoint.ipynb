{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3e1aef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import sys\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a40b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb307afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  authors_data = authors_data.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\902075073.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  authors_data = authors_data.reset_index().drop('index',1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rogue Bureaucrats at Homeland Security Leak Re...</td>\n",
       "      <td>Rogue bureaucrats at the Department of Homelan...</td>\n",
       "      <td>Michael Patrick Leahy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA’s Scott Pruitt Follows Through on Promise ...</td>\n",
       "      <td>Environmental Protection Agency (EPA) Administ...</td>\n",
       "      <td>Michael Patrick Leahy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virginia Refuses to Track Refugee TB, State’s ...</td>\n",
       "      <td>The state of Virginia refuses to track the num...</td>\n",
       "      <td>Michael Patrick Leahy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Report: ’72 Terrorists Came From Countries Cov...</td>\n",
       "      <td>“A review of information compiled by a Senate ...</td>\n",
       "      <td>Michael Patrick Leahy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dept of Homeland Security Inspector General Tr...</td>\n",
       "      <td>Department of Homeland Security Inspector Gene...</td>\n",
       "      <td>Michael Patrick Leahy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Rogue Bureaucrats at Homeland Security Leak Re...   \n",
       "1  EPA’s Scott Pruitt Follows Through on Promise ...   \n",
       "2  Virginia Refuses to Track Refugee TB, State’s ...   \n",
       "3  Report: ’72 Terrorists Came From Countries Cov...   \n",
       "4  Dept of Homeland Security Inspector General Tr...   \n",
       "\n",
       "                                             content                 author  \n",
       "0  Rogue bureaucrats at the Department of Homelan...  Michael Patrick Leahy  \n",
       "1  Environmental Protection Agency (EPA) Administ...  Michael Patrick Leahy  \n",
       "2  The state of Virginia refuses to track the num...  Michael Patrick Leahy  \n",
       "3  “A review of information compiled by a Senate ...  Michael Patrick Leahy  \n",
       "4  Department of Homeland Security Inspector Gene...  Michael Patrick Leahy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make a DataFrame with articles by our chosen authors\n",
    "# Include author names and article titles.\n",
    "\n",
    "# Make a list of the 10 chosen author names\n",
    "names = data.author.value_counts()[data.author.value_counts()>100][-30:].index.tolist()\n",
    "\n",
    "# DataFrame for articles of all chosen authors\n",
    "authors_data = pd.DataFrame()\n",
    "for name in names:\n",
    "    # Select each author's data\n",
    "    articles = data[data.author==name][:100][['title','content','author']]\n",
    "    # Append it to the DataFrame\n",
    "    authors_data = authors_data.append(articles)\n",
    "\n",
    "authors_data = authors_data.reset_index().drop('index',1)\n",
    "    \n",
    "authors_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43803804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 3000\n",
      "Unique articles: 3000\n",
      "Unique authors: 30\n",
      "\n",
      "Articles by author:\n",
      "\n",
      "Michael Patrick Leahy    100\n",
      "Nick Hallett             100\n",
      "Oliver Darcy             100\n",
      "Daniella Diaz            100\n",
      "Eli Watkins              100\n",
      "Jeremy Berke             100\n",
      "Julie Bort               100\n",
      "Raheem Kassam            100\n",
      "Laura Smith-Spark        100\n",
      "Eugene Scott             100\n",
      "Kevin Liptak             100\n",
      "Scott Davis              100\n",
      "Robert J. Marlow         100\n",
      "Biz Carson               100\n",
      "Dylan Gwinn              100\n",
      "Bryan Logan              100\n",
      "John Nolte               100\n",
      "Matthew DeBord           100\n",
      "Lee Stranahan            100\n",
      "Kipp Jones               100\n",
      "Theodore Schleifer       100\n",
      "Matt Weinberger          100\n",
      "Bob Bryan                100\n",
      "Ildefonso Ortiz          100\n",
      "Penny Starr              100\n",
      "James Delingpole         100\n",
      "Stephen Collinson        100\n",
      "Lana Shadwick            100\n",
      "Lisa Respers France      100\n",
      "Cartel Chronicles        100\n",
      "Name: author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look for duplicates\n",
    "print('Number of articles:',authors_data.shape[0])\n",
    "print('Unique articles:',len(np.unique(authors_data.index)))\n",
    "\n",
    "# Number of authors\n",
    "print('Unique authors:',len(np.unique(authors_data.author)))\n",
    "print('')\n",
    "print('Articles by author:\\n')\n",
    "\n",
    "# Articles counts by author\n",
    "print(authors_data.author.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900a5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of common words: 6311\n",
      "done in 137.393s\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m spacy download en\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Load spacy NLP object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# A list to store common words by all authors\n",
    "common_words = []\n",
    "\n",
    "# A dictionary to store the spacy_doc object of each author\n",
    "authors_docs = {}\n",
    "\n",
    "for name in names:\n",
    "    # Corpus is all the text written by that author\n",
    "    corpus = \"\"\n",
    "    # Grab all rows of current author, along the 'content' column\n",
    "    author_content = authors_data.loc[authors_data.author==name,'content']\n",
    "    \n",
    "    # Merge all articles in to the author's corpus\n",
    "    for article in author_content:\n",
    "        corpus = corpus + article\n",
    "    # Let Spacy parse the author's body of text\n",
    "    doc = nlp(corpus)\n",
    "    \n",
    "    # Store the doc in the dictionary\n",
    "    authors_docs[name] = doc\n",
    "        \n",
    "    # Filter out punctuation and stop words.\n",
    "    lemmas = [token.lemma_ for token in doc\n",
    "                if not token.is_punct and not token.is_stop]\n",
    "        \n",
    "    # Return the most common words of that author's corpus.\n",
    "    bow = [item[0] for item in Counter(lemmas).most_common(1000)]\n",
    "    \n",
    "    # Add them to the list of words by all authors.\n",
    "    for word in bow:\n",
    "        common_words.append(word)\n",
    "\n",
    "# Eliminate duplicates\n",
    "common_words = set(common_words)\n",
    "    \n",
    "print('Total number of common words:',len(common_words))\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c990a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Patrick Leahy corpus contains 135674  words.\n",
      "Nick Hallett corpus contains 42618  words.\n",
      "Lisa Respers France corpus contains 49557  words.\n",
      "Lana Shadwick corpus contains 74751  words.\n",
      "Stephen Collinson corpus contains 151144  words.\n",
      "James Delingpole corpus contains 93674  words.\n",
      "Penny Starr corpus contains 52935  words.\n",
      "Ildefonso Ortiz corpus contains 44085  words.\n",
      "Bob Bryan corpus contains 50974  words.\n",
      "Matt Weinberger corpus contains 44231  words.\n",
      "Theodore Schleifer corpus contains 59431  words.\n",
      "Kipp Jones corpus contains 55305  words.\n",
      "Lee Stranahan corpus contains 92080  words.\n",
      "Matthew DeBord corpus contains 73733  words.\n",
      "John Nolte corpus contains 52087  words.\n",
      "Bryan Logan corpus contains 40825  words.\n",
      "Dylan Gwinn corpus contains 43435  words.\n",
      "Biz Carson corpus contains 49997  words.\n",
      "Robert J. Marlow corpus contains 42876  words.\n",
      "Scott Davis corpus contains 39018  words.\n",
      "Kevin Liptak corpus contains 93057  words.\n",
      "Eugene Scott corpus contains 66216  words.\n",
      "Laura Smith-Spark corpus contains 100078  words.\n",
      "Raheem Kassam corpus contains 74207  words.\n",
      "Julie Bort corpus contains 64719  words.\n",
      "Jeremy Berke corpus contains 37468  words.\n",
      "Eli Watkins corpus contains 74733  words.\n",
      "Daniella Diaz corpus contains 48287  words.\n",
      "Oliver Darcy corpus contains 57192  words.\n",
      "Cartel Chronicles corpus contains 46935  words.\n"
     ]
    }
   ],
   "source": [
    "# Let's see our 10 authors in the dictionary\n",
    "lengths = []\n",
    "for k,v in authors_docs.items():\n",
    "    print(k,'corpus contains',len(v),' words.')\n",
    "    lengths.append(len(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633165de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all common_words: 6311\n",
      "Count of lowercase common_words: 3632\n",
      "Count of lowercase common_words (After Conversion): 6195\n"
     ]
    }
   ],
   "source": [
    "# check for lower case words\n",
    "common_words = pd.Series(pd.DataFrame(columns=common_words).columns)\n",
    "print('Count of all common_words:',len(common_words))\n",
    "print('Count of lowercase common_words:',np.sum([word.islower() for word in common_words]))\n",
    "\n",
    "# Turn all common_words into lower case\n",
    "common_words = [word.lower() for word in common_words]\n",
    "print('Count of lowercase common_words (After Conversion):',np.sum([word.islower() for word in common_words]))\n",
    "\n",
    "\n",
    "# We must remove these in to avoid conflicts with existing features.\n",
    "if 'author' in common_words:\n",
    "    common_words.remove('author')\n",
    "if 'title' in common_words:\n",
    "    common_words.remove('title')\n",
    "if 'content' in common_words:\n",
    "    common_words.remove('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ede7cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bow_counts = bow_counts.append(articles)\n",
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2183598352.py:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  bow_counts = bow_counts.reset_index().drop('index',1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article  0  done after  0.012963008880615235  minutes.\n",
      "Article  50  done after  0.8114549120267233  minutes.\n",
      "Article  100  done after  1.0466927369435628  minutes.\n",
      "Article  150  done after  1.3005126476287843  minutes.\n",
      "Article  200  done after  1.7675174117088317  minutes.\n",
      "Article  250  done after  2.6322685360908507  minutes.\n",
      "Article  300  done after  3.1459436655044555  minutes.\n",
      "Article  350  done after  3.491472363471985  minutes.\n",
      "Article  400  done after  3.760651405652364  minutes.\n",
      "Article  450  done after  4.104629588127136  minutes.\n",
      "Article  500  done after  4.344528261820475  minutes.\n",
      "Article  550  done after  4.656891612211863  minutes.\n",
      "Article  600  done after  5.004312598705292  minutes.\n",
      "Article  650  done after  5.550944721698761  minutes.\n",
      "Article  700  done after  5.99284686644872  minutes.\n",
      "Article  750  done after  6.287812872727712  minutes.\n",
      "Article  800  done after  6.5805663704872135  minutes.\n",
      "Article  850  done after  6.851781483491262  minutes.\n",
      "Article  900  done after  7.129324694474538  minutes.\n",
      "Article  950  done after  7.394613885879517  minutes.\n",
      "Article  1000  done after  7.637783932685852  minutes.\n",
      "Article  1050  done after  8.218176356951396  minutes.\n",
      "Article  1100  done after  8.617391149202982  minutes.\n",
      "Article  1150  done after  9.260394132137298  minutes.\n",
      "Article  1200  done after  9.74877405166626  minutes.\n",
      "Article  1250  done after  10.198139242331187  minutes.\n",
      "Article  1300  done after  10.454885621865591  minutes.\n",
      "Article  1350  done after  10.884541968504587  minutes.\n",
      "Article  1400  done after  11.16798597574234  minutes.\n",
      "Article  1450  done after  11.605903319517772  minutes.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of times a common_word appears in each article\n",
    "# (about 3Hrs processing)\n",
    "\n",
    "bow_counts = pd.DataFrame()\n",
    "for name in names:\n",
    "    # Select X articles of that author\n",
    "    articles = authors_data.loc[authors_data.author==name,:][:50]\n",
    "    bow_counts = bow_counts.append(articles)\n",
    "bow_counts = bow_counts.reset_index().drop('index',1)\n",
    "\n",
    "# Use common_words as the columns of a temporary DataFrame\n",
    "df = pd.DataFrame(columns=common_words)\n",
    "\n",
    "# Join BOW features with the author's content\n",
    "bow_counts = bow_counts.join(df)\n",
    "\n",
    "# Initialize rows with zeroes\n",
    "bow_counts = bow_counts.loc[:,~bow_counts.columns.duplicated()].copy()\n",
    "bow_counts.loc[:,common_words] = 0\n",
    "\n",
    "# Fill the DataFrame with counts of each feature in each article\n",
    "t0 = time()\n",
    "for i, article in enumerate(bow_counts.content):\n",
    "    doc = nlp(article)\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in common_words:\n",
    "            bow_counts.loc[i,token.lemma_.lower()] += 1\n",
    "    # Print a message every X articles\n",
    "    if i % 50 == 0:\n",
    "        if time()-t0 < 3600: # if less than an hour in seconds\n",
    "            print(\"Article \",i,\" done after \",(time()-t0)/60,' minutes.')\n",
    "        else:\n",
    "            print(\"Article \",i,\" done after \",(time()-t0)/60/60,' hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1f04a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves the long-awaited data into a pickle file for easy recovery\n",
    "bow_counts.to_pickle('bow_counts')\n",
    "# Read it back in with the following\n",
    "#bow_counts = pd.read_pickle('bow_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4a2b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans, kmeans_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7d00a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\4097121179.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = bow_counts.drop(['content','author','title'], 1)\n"
     ]
    }
   ],
   "source": [
    "y = bow_counts['author']\n",
    "X = bow_counts.drop(['content','author','title'], 1)\n",
    "X = X.loc[:,~X.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ada5d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b50dfaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cpu..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 22it [00:10,  2.04it/s, center_shift=0.000000, iteration=22, tol=0.000100]   \n"
     ]
    }
   ],
   "source": [
    "num_clusters = 30\n",
    "test = torch.from_numpy(test)\n",
    "device = torch.device('cpu')\n",
    "cluster_ids_x, cluster_centers = kmeans(\n",
    "    X=test, num_clusters=num_clusters, distance='euclidean', device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "741db993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25,  7, 11,  ..., 28, 28, 28])\n",
      "tensor([[0.6271, 0.0169, 0.0508,  ..., 0.0000, 0.0000, 0.0847],\n",
      "        [1.0500, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],\n",
      "        [0.6429, 0.2143, 0.2143,  ..., 0.0000, 0.0000, 0.2143],\n",
      "        ...,\n",
      "        [0.3878, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1026, 0.0897, 0.0128,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0714]])\n"
     ]
    }
   ],
   "source": [
    "print(cluster_ids_x)\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1ed34ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "newy = np.zeros(len(y))\n",
    "last = y[0]\n",
    "for i in range(len(y)):\n",
    "    if y[i] != last:\n",
    "        counter+=1\n",
    "    newy[i] = counter\n",
    "    last = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33f3b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "333d50cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\penna\\AppData\\Local\\Temp\\ipykernel_13364\\2158714863.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = bow_counts.drop(['content','author','title'], 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [171]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     16\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(M, n_hidden_units), \u001b[38;5;66;03m#M features to H hiden units\u001b[39;00m\n\u001b[0;32m     17\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mTanh(),   \u001b[38;5;66;03m# 1st transfer function,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(n_hidden_units, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;66;03m# H hidden units to 1 output neuron\u001b[39;00m\n\u001b[0;32m     19\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSigmoid(), \u001b[38;5;66;03m# final tranfer function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m                     )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#loss_fn = torch.nn.MSELoss\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,D_in,H,D_out):\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Net,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "y = bow_counts['author']\n",
    "X = bow_counts.drop(['content','author','title'], 1)\n",
    "X = X.loc[:,~X.columns.duplicated()].copy()\n",
    "X = X.to_numpy()\n",
    "N, M = X.shape\n",
    "import torch.nn as nn\n",
    "#X = scipy.stats.zscore(X)\n",
    "\n",
    "n_hidden_units = 2     # number of hidden units\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 10000         # stop criterion 2 (max epochs in training)\n",
    "\n",
    "# K-fold crossvalidation\n",
    "#K = 3                   # only five folds to speed up this example\n",
    "#CV = model_selection.KFold(K, shuffle=True)\n",
    "model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    torch.nn.Tanh(),   # 1st transfer function,\n",
    "                    torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Sigmoid(), # final tranfer function\n",
    "                    )\n",
    "#loss_fn = torch.nn.MSELoss\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.linear2=nn.Linear(H,D_out)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=torch.sigmoid(self.linear1(x))  \n",
    "        x=self.linear2(x)\n",
    "        return x\n",
    "model=Net(30,25,30)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b76be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, newy, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6cf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = torch.from_numpy(x_val)\n",
    "y_val = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x=torch.from_numpy(x_train)\n",
    "        self.y=torch.from_numpy(y_train)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "data_set=Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de630d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=DataLoader(dataset=data_set,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1000\n",
    "loss_list=[]\n",
    "\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in trainloader:\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x.float())\n",
    "        # calculate loss, da Cross Entropy benutzt wird muss ich in den loss Klassen vorhersagen, \n",
    "        # also Wahrscheinlichkeit pro Klasse. Das mach torch.max(y,1)[1])\n",
    "        loss=loss_fn(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2206e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
